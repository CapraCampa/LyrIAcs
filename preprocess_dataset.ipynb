{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "gl7onz0TuL7g"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "from tokenizers.trainers import WordPieceTrainer\n",
        "from tokenizers.normalizers import Lowercase\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "kGANJWrCuL7h"
      },
      "outputs": [],
      "source": [
        "# import kagglehub\n",
        "# path = kagglehub.dataset_download(\"mateibejan/multilingual-lyrics-for-genre-classification\")\n",
        "# print(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "cEq3yS8zuL7i"
      },
      "outputs": [],
      "source": [
        "# !!! Execute this cell to load the dataset if it is not already loaded\n",
        "\n",
        "import pandas as pd\n",
        "modified_path = path.replace(\"C:\\\\\", \"/\").replace(\"\\\\\", \"/\") + \"/train.csv\"\n",
        "df = pd.read_csv(modified_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "usFo1anZuL7i"
      },
      "outputs": [],
      "source": [
        "# df = pd.read_csv(\"lyrics_genre.csv\")\n",
        "# df.head()\n",
        "# print(len(df))\n",
        "# print(df.Genre.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKk6rxf_uL7i",
        "outputId": "b04d98d8-5845-4105-88e7-7c401b988f3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "210010\n"
          ]
        }
      ],
      "source": [
        "df = df[df['Language']=='en']\n",
        "columns_to_drop = ['Language']\n",
        "df = df.drop(columns=columns_to_drop)\n",
        "\n",
        "df = df.groupby(['Artist', 'Song']).agg({\n",
        "    'Lyrics': 'first',  # Take the first instance of the lyrics for each group\n",
        "    'Genre': lambda x: list(set(x))       # Aggregate all genres into a list\n",
        "}).reset_index()\n",
        "print(len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rilhv-8duL7i",
        "outputId": "ad89c4c7-40d1-4b75-cb1d-360d3da5378f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Artist          Song  \\\n",
            "122  2 chainz    100 joints   \n",
            "169  2 chainz         kesha   \n",
            "713   50 cent  all his love   \n",
            "714   50 cent     all of me   \n",
            "726   50 cent      bad news   \n",
            "\n",
            "                                                Lyrics           Genre  \n",
            "122  [Hook]\\nNo matter what they say I smoke 100 jo...  [Hip-Hop, Pop]  \n",
            "169  I am in love, with what we are, not what we sh...  [Hip-Hop, Pop]  \n",
            "713  [Chorus: singing]\\nHe gon' give you all his lo...  [Hip-Hop, Pop]  \n",
            "714  Mary~\\nnow if i give ya all of me what ya gon ...  [Hip-Hop, Pop]  \n",
            "726  Lloyd Banks in the house, bad news\\nTony Yayo ...  [Hip-Hop, Pop]  \n"
          ]
        }
      ],
      "source": [
        "multi_genre_songs = df[df['Genre'].apply(len) > 1]\n",
        "\n",
        "# Display these songs\n",
        "print(multi_genre_songs.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "zlxcXvBHuL7j"
      },
      "outputs": [],
      "source": [
        "columns_to_drop = ['Artist',\"Song\"]\n",
        "df = df.drop(columns=columns_to_drop)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def split_into_sentences(lyrics):\n",
        "    sentences = sent_tokenize(lyrics)\n",
        "    return '\\n'.join(sentences)\n",
        "\n",
        "df['Lyrics'] = df['Lyrics'].apply(split_into_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xL31Tuk8uq23",
        "outputId": "d7e56a29-0e1c-45f1-b03f-d7843711fa82"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['Lyrics'][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uKC4cSmxrPO",
        "outputId": "f8466669-493e-41cc-ad02-8d6db277bbeb"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hazy Days With Lazy Ways You get less done but more Out of your days How can you work yet avoid Getting out of your lazy ways You'll never get up if you don't get up You'll always stay down if you sit around You'll never get up if you don't get up You'll always stay down if you sit around Hazy Days Lazy Ways We got less done but more Out of our days How can we ever recapture the feeling Of lazy ways You'll never get up if you don't get up You'll always stay down if you sit around Where nobody cares and nobody tries 'Cos a daydreams resting on the back of your eyes On the back of your eyes Taking five Bring love to me With your body Let me hold you Endless, endlessly You'll never get up if you don't get up You'll always stay down if you sit around Where nobody cares and nobody tries 'Cos a daydreams resting on the back of your eyes On the back of your eyes Hazy days Lazy ways You get less done but more Out of your days Crazy days Lazy ways\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RkCpYMbuL7j",
        "outputId": "2e9d0ae0-0555-4039-b9e2-15393c8e0f57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hazy days with lazy ways you get less done but more out of your days how can you work yet avoid getting out of your lazy ways youll never get up if you dont get up youll always stay down if you sit around youll never get up if you dont get up youll always stay down if you sit around hazy days lazy ways we got less done but more out of our days how can we ever recapture the feeling of lazy ways youll never get up if you dont get up youll always stay down if you sit around where nobody cares and nobody tries cos a daydreams resting on the back of your eyes on the back of your eyes taking five bring love to me with your body let me hold you endless endlessly youll never get up if you dont get up youll always stay down if you sit around where nobody cares and nobody tries cos a daydreams resting on the back of your eyes on the back of your eyes hazy days lazy ways you get less done but more out of your days crazy days lazy ways\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "# to reduce vocab size let s try to remove more unecessary symbols and parts\n",
        "# Define a cleaning function\n",
        "def clean_lyrics(lyrics):\n",
        "    # Remove section tags like [Intro], [Verse 1], etc.\n",
        "    cleaned = re.sub(r'\\[.*?\\]', '', lyrics)\n",
        "\n",
        "    # Remove credits or text after \"---\"\n",
        "    cleaned = re.split(r'---', cleaned)[0]\n",
        "\n",
        "    # Remove symbols except for line breaks, alphanumeric characters, and key punctuation\n",
        "    cleaned = re.sub(r\"[^\\w\\s\\n.!?]\", '', cleaned)\n",
        "\n",
        "    # Remove extra whitespace and blank lines\n",
        "    cleaned = re.sub(r'\\n\\s*\\n', '\\n', cleaned).strip()\n",
        "\n",
        "    cleaned = cleaned.lower()\n",
        "\n",
        "    # Repeated punctuation sign normalization\n",
        "    cleaned = re.sub(r'(\\!{2,})', ' exclamations ', cleaned)\n",
        "    cleaned = re.sub(r'(\\?{2,})', ' questionMarks ', cleaned)\n",
        "    cleaned = re.sub(r'(\\.{3,})', ' ellipsis ', cleaned)\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "# Apply the cleaning function\n",
        "df['Lyrics'] = df['Lyrics'].apply(clean_lyrics)\n",
        "print(df['Lyrics'][1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['Lyrics'][12])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebTIWCKUw3n7",
        "outputId": "a84f76a3-ac72-495d-d9bd-cd6c6784f7d0"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "times are far between and few i bet we can look upon our lives without regret of all the things i have done you think im proud of everyone without exception til you make your peace with yesterday never build a future i swear by what i say whatever penance you do decide what its worth to you and then respect it however long it will take to weather your mistakes why not accept it?\n",
            "my hands for now are tied im a body frozen im a will thats paralyzed when will you ever set aside your pain and misery?\n",
            "no matter how i beg no matter how i wish or plead youll never be more than alive youll never do more than survive until you expect it do you want to build a world with our lives?\n",
            "you better soon decide or you can forget it my hands for now are tied im a body frozen im a will thats paralyzed til you drop that heavy baggage youre dragging behind there wont be room for us to both go this ride\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ySHrCMyhuL7j"
      },
      "outputs": [],
      "source": [
        "# def handle_negations(text):\n",
        "#     sub_tokens = text.split()\n",
        "#     neg = False\n",
        "#     neg_tokens = []\n",
        "#     punctuation = ['.', '?', '!', ';', 'exclamations', 'questionMarks', 'ellipsis']\n",
        "#     negation_words = [\"n't\", \"not\", \"no\", \"never\"]\n",
        "\n",
        "#     for token in sub_tokens:\n",
        "#         if token in punctuation:\n",
        "#             neg = False\n",
        "\n",
        "#         neg_token = f\"NOT_{token}\" if neg else token\n",
        "\n",
        "#         if '\\'t' in token or token == 'not':\n",
        "#             neg = True\n",
        "\n",
        "#         if any(neg_word in token for neg_word in negation_words):\n",
        "#             neg = True\n",
        "\n",
        "#         neg_tokens.append(neg_token)\n",
        "\n",
        "\n",
        "#     return ' '.join(neg_tokens)\n",
        "\n",
        "# df['Lyrics'] = df['Lyrics'].apply(handle_negations)\n",
        "\n",
        "# print(df['Lyrics'][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNXed1YxuL7j",
        "outputId": "e97d42f6-143c-41ba-9b5a-21d0b8ed8eed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "well old dan tanengraph you can keep your cold dead stare your pretty ladys waiting out in the coal stained air and ive watched your hair turn grey throughout the coarse of one fateful day and she got shy water in her eyes and she got shy water in her eyes well on that creaky wooden porch you can think about what youve lost while some nurse you dont know brings you paper cups full of pills well your pretty ladys somewhere lonely and turning grey oh what a mistake you have made and she got shy water in her eyes and she got shy water in her eyes well you can stick to your guns if you please it dont bother me oh no you can stick to your guns if you need and you need well if theres a moral here thats to be learned its not to let a good thing pass you by well right now youre staring straight ahead beside some girl with shy water in her eyes and she got shy water in her eyes and she got shy water in her eyes well you can stick to your guns if you please it dont bother me oh no\n"
          ]
        }
      ],
      "source": [
        "print(df['Lyrics'][11])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkrVi1iguL7j",
        "outputId": "7c7abfc9-02e1-4b5c-f50e-2aea6771fdf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dynamic max_sequence_length: 529\n",
            "Top 10 most frequent words in tokenizer vocabulary: [('<UNK>', 1), ('the', 2), ('you', 3), ('i', 4), ('and', 5), ('to', 6), ('a', 7), ('me', 8), ('my', 9), ('it', 10)]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Step 2: Initialize the tokenizer with an OOV token\n",
        "tokenizer = Tokenizer(oov_token=\"<UNK>\")\n",
        "\n",
        "# Fit the tokenizer on the sampled lyrics data\n",
        "tokenizer.fit_on_texts(df['Lyrics'])\n",
        "\n",
        "# Step 3: Convert lyrics to sequences\n",
        "sequences = tokenizer.texts_to_sequences(df['Lyrics'])\n",
        "\n",
        "# Determine the longest sequence length dynamically\n",
        "sequence_lengths = np.array([len(seq) for seq in sequences])\n",
        "max_sequence_length = int(np.percentile(sequence_lengths, 95))\n",
        "print(f\"Dynamic max_sequence_length: {max_sequence_length}\")\n",
        "\n",
        "# Step 4: Pad or truncate sequences to match the dynamic length\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Print the top 10 most frequent words in the tokenizer's word index\n",
        "print(f\"Top 10 most frequent words in tokenizer vocabulary: {list(tokenizer.word_index.items())[:10]}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}